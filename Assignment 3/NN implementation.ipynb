{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d533cd",
   "metadata": {},
   "source": [
    "# Creation of an artificial neural network from scratch\n",
    "\n",
    "This notebook currently contains a very basic implementation of an artificial neural network from scratch. \n",
    "The first cell contains\n",
    "\n",
    "1. A definition of several activation functions and their derivatives.\n",
    "2. A definition of a loss function and its derivative.\n",
    "3. A definition of the forward propagation through a single layer\n",
    "4. A definition of a backward propagation through a single layer\n",
    "\n",
    "\n",
    "Your task is to do the following extentions to the code in this cell:\n",
    "\n",
    "1. Add support for an additional activation function.\n",
    "2. Add the use of a bias in the forward- and backward-propagation. \n",
    "As it is implemented now, all solutions have to pass the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32b521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(12345)  # Set initial random seed (good to always do)\n",
    "\n",
    "# Activation functions, the parameter f decides which activation function that is used.\n",
    "# Added: Sigmoid activation function and its derivative\n",
    "def activate(a, f=\"none\"):\n",
    "    if f == \"ReLU\":\n",
    "        y = ReLU(a)\n",
    "    elif f == \"softmax\":\n",
    "        y = softmax(a)\n",
    "    elif f == \"sigmoid\":\n",
    "        y = 1 / (1 + np.exp(-a))\n",
    "    else:\n",
    "        y = a\n",
    "    return y\n",
    "\n",
    "\n",
    "# Derivatives of activation functions\n",
    "def d_activate(a, f=\"none\"):\n",
    "    if f == \"none\":\n",
    "        dy = np.ones_like(a)\n",
    "    elif f == \"ReLU\":\n",
    "        dy = 1 * (a > 0)\n",
    "    elif f == \"sigmoid\":\n",
    "        dy = activate(a, \"sigmoid\") * (1 - activate(a, \"sigmoid\"))\n",
    "    elif f == \"softmax\":\n",
    "        dy = 1\n",
    "\n",
    "    # print(f)\n",
    "    return dy\n",
    "\n",
    "\n",
    "# This is the loss for a set of predictions y_hat compared to a set of real values y\n",
    "def MSE_loss(y_hat, y):\n",
    "    return 1/2 * np.mean((y_hat-y)**2)\n",
    "\n",
    "# This is the derivative of the loss with respect to the predicted value y_hat\n",
    "def d_MSE_loss(y_hat, y):\n",
    "    m = y.shape[0]\n",
    "    return 1./m * (y_hat-y)\n",
    "\n",
    "\n",
    "# Propagate a signal through a layer in a neural network.\n",
    "# Added: support for the use of a bias\n",
    "def propagate_forward(w, b, a, f=\"none\"):\n",
    "    z = activate(np.dot(a, w) + b, f)\n",
    "    return z\n",
    "\n",
    "# Calculate the backward gradients that are passed through the layer in the backward pass.\n",
    "# Returns both the derivative of the loss with respect to the weights w and the input signal a.\n",
    "# Added: support for a bias\n",
    "def propagate_backward(w, b, a, dl_dz, f=\"none\"):\n",
    "    dl = d_activate(np.dot(a, w) + b, f) * dl_dz\n",
    "    dw = np.dot(a.T, dl)\n",
    "    db = np.sum(dl, axis=0)\n",
    "    da = np.dot(dl, w.T)\n",
    "    return dw, db, da\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_ReLU(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "\n",
    "def d_softmax_cross_entropy(x, y):\n",
    "    return x - y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73645779",
   "metadata": {},
   "source": [
    "## An implementation of a neural network\n",
    "\n",
    "Below is an implementation of a MLP neural network. This implementation is still lacking several details that are needed for the network to be robust and function well. Your task is to improve it with the following:\n",
    "\n",
    "1. Add a bias to the activation functions that are fine tuned during training. \n",
    "2. Add a function that trains the network using minibatches (the network only trains on a few samples at a time) \n",
    "3. Optional: Make use of an validation set in the training function. The model should hence stop training when the loss starts to increase for the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c550aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "class NeuralNet(object):\n",
    "    # Setup all parameters and activation functions.\n",
    "    # Added: support for a bias for each neuron\n",
    "    def __init__(self, input_size, num_classes, hidden_neurons, hidden_activations=[], output_activation='softmax'):\n",
    "        self.ws = []\n",
    "        self.bs = []\n",
    "        self.activations = []\n",
    "        self.output_dim = num_classes\n",
    "\n",
    "        last_size = input_size\n",
    "        for neurons, activation in zip(hidden_neurons, hidden_activations):\n",
    "            self.ws.append(np.random.randn(last_size, neurons) * np.sqrt(2 / last_size))\n",
    "            self.bs.append(np.zeros(neurons))\n",
    "            self.activations.append(activation)\n",
    "            last_size = neurons\n",
    "\n",
    "        self.ws.append(np.random.randn(last_size, num_classes) * np.sqrt(2 / last_size))\n",
    "        self.bs.append(np.zeros(num_classes))\n",
    "        self.activations.append(output_activation)\n",
    "\n",
    "        # Print statements to check the output layer\n",
    "        # print(f\"Output layer weights shape: {self.ws[-1].shape}\")\n",
    "        # print(f\"Output layer bias shape: {self.bs[-1].shape}\")\n",
    "        # print(f\"Output layer activation: {self.activations[-1]}\")\n",
    "\n",
    "    # Predict the input through the network and calculate the output.\n",
    "    # Added: support for a bias for each neuron\n",
    "    def forward(self, x):\n",
    "        for w, b, f in zip(self.ws, self.bs, self.activations):\n",
    "            x = propagate_forward(w, b, x, f)\n",
    "        return x\n",
    "\n",
    "    # Adjust the weights in the network to better fit the desired output (y), given the input (x).\n",
    "    # alpha is the learning rate.\n",
    "    # Added: support for a bias for each neuron and make sure these are learned as well.\n",
    "    def adjust_weights(self, x, y, alpha=1e-4):\n",
    "        a = x\n",
    "        al = []\n",
    "\n",
    "        for w, b, f in zip(self.ws, self.bs, self.activations):\n",
    "            al.append(a)\n",
    "            a = propagate_forward(w, b, a, f)\n",
    "            loss = MSE_loss(a, y)\n",
    "            da = d_MSE_loss(a, y)\n",
    "\n",
    "        for w, b, f, x in reversed(list(zip(self.ws, self.bs, self.activations, al))):\n",
    "            dw, db, da = propagate_backward(w, b, x, da, f)\n",
    "            w -= alpha * dw\n",
    "            b -= alpha * db\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # A function for the training of the network.\n",
    "    # Added: a training loop, support for mini batches, and optional training/validation data split\n",
    "    def train_net(self, x, y, batch_size=64, epochs=100, val_split=0.1, verbose=True):\n",
    "        n = x.shape[0]\n",
    "        num_batches = n // batch_size\n",
    "        val_size = int(n * val_split)\n",
    "        train_size = n - val_size\n",
    "\n",
    "        x_train, x_val = x[:train_size], x[train_size:]\n",
    "        y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in range(num_batches):\n",
    "                start = batch * batch_size\n",
    "                end = min((batch + 1) * batch_size, train_size)\n",
    "                x_batch = x_train[start:end]\n",
    "                y_batch = y_train[start:end]\n",
    "\n",
    "                #     # Add print statements here\n",
    "                # print(f\"x_batch shape: {x_batch.shape}\")\n",
    "                # print(f\"y_batch shape: {y_batch.shape}\")\n",
    "                # print(f\"y_batch sample: {y_batch[:5]}\")\n",
    "\n",
    "            \n",
    "                loss = self.adjust_weights(x_batch, y_batch)\n",
    "                losses.append(loss)\n",
    "\n",
    "            if val_split > 0:\n",
    "                y_val_pred = self.forward(x_val)\n",
    "                val_loss = MSE_loss(y_val_pred, y_val)\n",
    "                val_losses.append(val_loss)\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch + 1}: Training Loss: {loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Epoch {epoch + 1}: Training Loss: {loss:.4f}\")\n",
    "\n",
    "        return losses, val_losses\n",
    "    \n",
    "    def accuracy(self, x, y):\n",
    "        y_pred = self.forward(x)\n",
    "\n",
    "        # get confusion matrix\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(y, axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        return cm\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ac988e0",
   "metadata": {},
   "source": [
    "# Misc Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf3e341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_preprocess_dataset(url, feature_cols, target_col, column_names=None, test_size=0.2):\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42, stratify=df[target_col])\n",
    "\n",
    "    # Create X and y arrays\n",
    "    X_train = train_df[feature_cols].values\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_train = pd.get_dummies(train_df[target_col]).values\n",
    "    y_test = pd.get_dummies(test_df[target_col]).values\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    num_classes = len(np.unique(labels))\n",
    "    one_hot_encoded = np.zeros((labels.shape[0], num_classes))\n",
    "    one_hot_encoded[np.arange(labels.shape[0]), labels.reshape(-1)] = 1\n",
    "    return one_hot_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbb2e0",
   "metadata": {},
   "source": [
    "## Main programs\n",
    "\n",
    "This cell should contain your different programs. In this cell you should present:\n",
    "\n",
    "1. At least 3 programs where the neural network is applied to 3 different datasets.\n",
    "2. You should also have at least 2 programs where you fine tune 2 hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0193eb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Iris dataset: [[10  0  0]\n",
      " [10  0  0]\n",
      " [10  0  0]]\n",
      "Test accuracy for Wine dataset: [[12  0  0]\n",
      " [10  4  0]\n",
      " [10  0  0]]\n",
      "Test accuracy for Breast Cancer dataset: [[72  0]\n",
      " [18 24]]\n"
     ]
    }
   ],
   "source": [
    "def main1():\n",
    "    np.random.seed(1234) # Set initial random seed (good to always do)\n",
    "    n = 1000\n",
    "    d = 4\n",
    "    k = np.random.randint(0, 10, (d, 1))\n",
    "    x = np.random.normal(0, 1, (n, d))\n",
    "    y = np.dot(x, k) + 0.1 + np.random.normal(0, 0.01, (n, 1))\n",
    "\n",
    "    nn = NeuralNet(d, 1, [18, 12])\n",
    "\n",
    "    losses, val_losses = nn.train_net(x, y, batch_size=64, epochs=100, val_split=0.1)\n",
    "\n",
    "    plt.plot(losses)\n",
    "    plt.plot(np.arange(0, len(losses), len(losses) // len(val_losses)), val_losses)\n",
    "    plt.title(\"Loss\")\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def iris():\n",
    "    iris_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "    iris_column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "    iris_feature_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "    iris_target_col = 'species'\n",
    "\n",
    "    X_train_iris, X_test_iris, y_train_iris, y_test_iris = load_and_preprocess_dataset(iris_url, iris_feature_cols, iris_target_col, column_names=iris_column_names)\n",
    "\n",
    "    return X_train_iris, X_test_iris, y_train_iris, y_test_iris\n",
    "\n",
    "\n",
    "def wine():\n",
    "    wine_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "    wine_column_names = ['class', 'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
    "    wine_feature_cols = wine_column_names[1:]  # All columns except the first one\n",
    "    wine_target_col = 'class'\n",
    "\n",
    "    X_train_wine, X_test_wine, y_train_wine, y_test_wine = load_and_preprocess_dataset(wine_url, wine_feature_cols, wine_target_col, column_names=wine_column_names)\n",
    "\n",
    "    return X_train_wine, X_test_wine, y_train_wine, y_test_wine\n",
    "\n",
    "\n",
    "def bc():\n",
    "    bc_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "    bc_column_names = ['id', 'diagnosis'] + [f\"feature_{i}\" for i in range(30)]\n",
    "    bc_feature_cols = [f\"feature_{i}\" for i in range(30)]\n",
    "    bc_target_col = 'diagnosis'\n",
    "\n",
    "    X_train_bc, X_test_bc, y_train_bc, y_test_bc = load_and_preprocess_dataset(bc_url, bc_feature_cols, bc_target_col, column_names=bc_column_names)\n",
    "\n",
    "    return X_train_bc, X_test_bc, y_train_bc, y_test_bc\n",
    "\n",
    "\n",
    "\n",
    "def main2():\n",
    "    X_train_iris, X_test_iris, y_train_iris, y_test_iris = iris()\n",
    "    X_train_wine, X_test_wine, y_train_wine, y_test_wine = wine()\n",
    "    X_train_bc, X_test_bc, y_train_bc, y_test_bc = bc()\n",
    "\n",
    "    nn_iris = NeuralNet(X_train_iris.shape[1], y_train_iris.shape[1], hidden_neurons=[3], hidden_activations=[\"ReLU\"], output_activation='softmax')\n",
    "    nn_wine = NeuralNet(X_train_wine.shape[1], y_train_wine.shape[1], hidden_neurons=[3], hidden_activations=[\"ReLU\"], output_activation='softmax')\n",
    "    nn_bc = NeuralNet(X_train_bc.shape[1], y_train_bc.shape[1], hidden_neurons=[2], hidden_activations=[\"ReLU\"], output_activation='softmax')\n",
    "\n",
    "    # Train the neural networks\n",
    "    nn_iris.train_net(X_train_iris, y_train_iris, batch_size=64, epochs=100, val_split=0.1, verbose=False)\n",
    "    nn_wine.train_net(X_train_wine, y_train_wine, batch_size=64, epochs=100, val_split=0.1, verbose=False)\n",
    "    nn_bc.train_net(X_train_bc, y_train_bc, batch_size=64, epochs=100, val_split=0.1, verbose=False)\n",
    "\n",
    "    # Evaluate the performance\n",
    "    test_accuracy_iris = nn_iris.accuracy(X_test_iris, y_test_iris)\n",
    "    test_accuracy_wine = nn_wine.accuracy(X_test_wine, y_test_wine)\n",
    "    test_accuracy_bc = nn_bc.accuracy(X_test_bc, y_test_bc)\n",
    "\n",
    "    print(\"Test accuracy for Iris dataset:\", test_accuracy_iris)\n",
    "    print(\"Test accuracy for Wine dataset:\", test_accuracy_wine)\n",
    "    print(\"Test accuracy for Breast Cancer dataset:\", test_accuracy_bc)\n",
    "\n",
    "main2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1230f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
